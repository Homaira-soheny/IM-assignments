{"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.7.9","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"papermill":{"duration":47.674669,"end_time":"2020-10-30T19:06:57.543721","environment_variables":{},"exception":null,"input_path":"__notebook__.ipynb","output_path":"__notebook__.ipynb","parameters":{},"start_time":"2020-10-30T19:06:09.869052","version":"2.1.0"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"Run the next code cell without changes to use the data to train a simple model.  The output shows the accuracy of the model on some test data.","metadata":{"papermill":{"duration":0.012727,"end_time":"2020-10-30T19:06:45.774334","exception":false,"start_time":"2020-10-30T19:06:45.761607","status":"completed"},"tags":[]}},{"cell_type":"code","source":"# Set up feedback system\nfrom learntools.core import binder\nbinder.bind(globals())\nfrom learntools.ethics.ex3 import *\n\nimport numpy as np\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.feature_extraction.text import CountVectorizer\n\n# Get the same results each time\nnp.random.seed(0)\n\n# Load the (full) training data\nfull_data = pd.read_csv(\"../input/jigsaw-unintended-bias-in-toxicity-classification/train.csv\")\n\n# Work with a small subset of the data: if target > 0.7, toxic.  If target < 0.3, non-toxic\nfull_toxic = full_data[full_data[\"target\"]>0.7]\nfull_nontoxic = full_data[full_data[\"target\"]<0.3].sample(len(full_toxic))\ndata = pd.concat([full_toxic, full_nontoxic], ignore_index=True)\ncomments = data[\"comment_text\"]\ntarget = (data[\"target\"]>0.7).astype(int)\n\n# Break into training and test sets\ncomments_train, comments_test, y_train, y_test = train_test_split(comments, target, test_size=0.30, stratify=target)\n\n# Get vocabulary from training data\nvectorizer = CountVectorizer()\nvectorizer.fit(comments_train)\n\n# Get word counts for training and test sets\nX_train = vectorizer.transform(comments_train)\nX_test = vectorizer.transform(comments_test)\n\n# Preview the dataset\nprint(\"Data successfully loaded!\\n\")\nprint(\"Sample toxic comment:\", comments_train.iloc[18])\nprint(\"Sample not-toxic comment:\", comments_train.iloc[3])","metadata":{"execution":{"iopub.status.busy":"2021-06-25T16:29:10.682818Z","iopub.execute_input":"2021-06-25T16:29:10.683196Z","iopub.status.idle":"2021-06-25T16:29:40.628726Z","shell.execute_reply.started":"2021-06-25T16:29:10.683115Z","shell.execute_reply":"2021-06-25T16:29:40.626870Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"Data successfully loaded!\n\nSample toxic comment: Yeah, just like you, you moron.\nSample not-toxic comment: I'm lovin' it.\n","output_type":"stream"}]},{"cell_type":"code","source":"from sklearn.linear_model import LogisticRegression\n\n# Train a model and evaluate performance on test dataset\nclassifier = LogisticRegression(max_iter=2000)\nclassifier.fit(X_train, y_train)\nscore = classifier.score(X_test, y_test)\nprint(\"Accuracy:\", score)\n\n# Function to classify any string\ndef classify_string(string, investigate=False):\n    prediction = classifier.predict(vectorizer.transform([string]))[0]\n    if prediction == 0:\n        print(\"NOT TOXIC:\", string)\n    else:\n        print(\"TOXIC:\", string)","metadata":{"papermill":{"duration":11.047454,"end_time":"2020-10-30T19:06:56.836704","exception":false,"start_time":"2020-10-30T19:06:45.78925","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2021-06-25T16:29:40.630217Z","iopub.execute_input":"2021-06-25T16:29:40.630458Z","iopub.status.idle":"2021-06-25T16:30:08.379824Z","shell.execute_reply.started":"2021-06-25T16:29:40.630434Z","shell.execute_reply":"2021-06-25T16:30:08.378554Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stdout","text":"Accuracy: 0.9294488650947893\n","output_type":"stream"}]},{"cell_type":"markdown","source":"Roughly 93% of the comments in the test data are classified correctly!\n\n","metadata":{"papermill":{"duration":0.014849,"end_time":"2020-10-30T19:06:56.868884","exception":false,"start_time":"2020-10-30T19:06:56.854035","status":"completed"},"tags":[]}},{"cell_type":"code","source":"# Comment to pass through the model\nmy_comment = \"i hate orange\"\n\n# Do not change the code below\nclassify_string(my_comment)\nq_1.check()","metadata":{"papermill":{"duration":0.026189,"end_time":"2020-10-30T19:06:56.90901","exception":false,"start_time":"2020-10-30T19:06:56.882821","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2021-06-25T16:30:08.382080Z","iopub.execute_input":"2021-06-25T16:30:08.384471Z","iopub.status.idle":"2021-06-25T16:30:08.406108Z","shell.execute_reply.started":"2021-06-25T16:30:08.384415Z","shell.execute_reply":"2021-06-25T16:30:08.405033Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stdout","text":"NOT TOXIC: i hate orange\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.Javascript object>","application/javascript":"parent.postMessage({\"jupyterEvent\": \"custom.exercise_interaction\", \"data\": {\"outcomeType\": 1, \"valueTowardsCompletion\": 0.14285714285714285, \"interactionType\": 1, \"questionType\": 4, \"questionId\": \"1_TryOut\", \"learnToolsVersion\": \"0.3.4\", \"failureMessage\": \"\", \"exceptionClass\": \"\", \"trace\": \"\"}}, \"*\")"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"","text/markdown":"<span style=\"color:#33cc33\"></span>"},"metadata":{}}]},{"cell_type":"markdown","source":"The model assigns each of roughly 58,000 words a coefficient, where higher coefficients denote words that the model thinks are more toxic.  The code cell outputs the ten words that are considered most toxic, along with their coefficients.  ","metadata":{"papermill":{"duration":0.018539,"end_time":"2020-10-30T19:06:56.942426","exception":false,"start_time":"2020-10-30T19:06:56.923887","status":"completed"},"tags":[]}},{"cell_type":"code","source":"coefficients = pd.DataFrame({\"word\": sorted(list(vectorizer.vocabulary_.keys())), \"coeff\": classifier.coef_[0]})\ncoefficients.sort_values(by=['coeff']).tail(10)","metadata":{"papermill":{"duration":0.115908,"end_time":"2020-10-30T19:06:57.07637","exception":false,"start_time":"2020-10-30T19:06:56.960462","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2021-06-25T16:30:39.128239Z","iopub.execute_input":"2021-06-25T16:30:39.128595Z","iopub.status.idle":"2021-06-25T16:30:39.191739Z","shell.execute_reply.started":"2021-06-25T16:30:39.128564Z","shell.execute_reply":"2021-06-25T16:30:39.191063Z"},"trusted":true},"execution_count":4,"outputs":[{"execution_count":4,"output_type":"execute_result","data":{"text/plain":"            word     coeff\n25848  hypocrite  6.218425\n16985       dumb  6.446760\n12995       crap  6.519400\n34285      moron  6.626977\n38378   pathetic  6.643456\n26015    idiotic  6.668968\n49888  stupidity  7.503371\n26021     idiots  8.549837\n26013      idiot  8.636675\n49876     stupid  9.368592","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>word</th>\n      <th>coeff</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>25848</th>\n      <td>hypocrite</td>\n      <td>6.218425</td>\n    </tr>\n    <tr>\n      <th>16985</th>\n      <td>dumb</td>\n      <td>6.446760</td>\n    </tr>\n    <tr>\n      <th>12995</th>\n      <td>crap</td>\n      <td>6.519400</td>\n    </tr>\n    <tr>\n      <th>34285</th>\n      <td>moron</td>\n      <td>6.626977</td>\n    </tr>\n    <tr>\n      <th>38378</th>\n      <td>pathetic</td>\n      <td>6.643456</td>\n    </tr>\n    <tr>\n      <th>26015</th>\n      <td>idiotic</td>\n      <td>6.668968</td>\n    </tr>\n    <tr>\n      <th>49888</th>\n      <td>stupidity</td>\n      <td>7.503371</td>\n    </tr>\n    <tr>\n      <th>26021</th>\n      <td>idiots</td>\n      <td>8.549837</td>\n    </tr>\n    <tr>\n      <th>26013</th>\n      <td>idiot</td>\n      <td>8.636675</td>\n    </tr>\n    <tr>\n      <th>49876</th>\n      <td>stupid</td>\n      <td>9.368592</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"markdown","source":"None of the words are surprising. They are all clearly toxic.","metadata":{}},{"cell_type":"markdown","source":"# A closer investigation\n\nWe'll take a closer look at how the model classifies comments.\n","metadata":{"papermill":{"duration":0.015333,"end_time":"2020-10-30T19:06:57.175887","exception":false,"start_time":"2020-10-30T19:06:57.160554","status":"completed"},"tags":[]}},{"cell_type":"code","source":"# Set the value of new_comment\nnew_comment = \"I have a christian friend\"\n\n# Do not change the code below\nclassify_string(new_comment)\ncoefficients[coefficients.word.isin(new_comment.split())]\n","metadata":{"papermill":{"duration":0.044861,"end_time":"2020-10-30T19:06:57.236559","exception":false,"start_time":"2020-10-30T19:06:57.191698","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2021-06-25T16:32:56.198567Z","iopub.execute_input":"2021-06-25T16:32:56.198913Z","iopub.status.idle":"2021-06-25T16:32:56.217665Z","shell.execute_reply.started":"2021-06-25T16:32:56.198883Z","shell.execute_reply":"2021-06-25T16:32:56.216416Z"},"trusted":true},"execution_count":8,"outputs":[{"name":"stdout","text":"NOT TOXIC: I have a christian friend\n","output_type":"stream"},{"execution_count":8,"output_type":"execute_result","data":{"text/plain":"            word     coeff\n10378  christian  0.260963\n21422     friend  0.056005\n24208       have -0.066235","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>word</th>\n      <th>coeff</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>10378</th>\n      <td>christian</td>\n      <td>0.260963</td>\n    </tr>\n    <tr>\n      <th>21422</th>\n      <td>friend</td>\n      <td>0.056005</td>\n    </tr>\n    <tr>\n      <th>24208</th>\n      <td>have</td>\n      <td>-0.066235</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"markdown","source":"# Identify bias\n\nLet's run the comment \"I have a muslim friend\" and see the prediction of the model","metadata":{"papermill":{"duration":0.016152,"end_time":"2020-10-30T19:06:57.26916","exception":false,"start_time":"2020-10-30T19:06:57.253008","status":"completed"},"tags":[]}},{"cell_type":"code","source":"# Set the value of new_comment\nnew_comment = \"I have a muslim friend\"\n\n# Do not change the code below\nclassify_string(new_comment)\ncoefficients[coefficients.word.isin(new_comment.split())]\n","metadata":{"papermill":{"duration":0.024534,"end_time":"2020-10-30T19:06:57.310158","exception":false,"start_time":"2020-10-30T19:06:57.285624","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2021-06-25T16:34:42.389849Z","iopub.execute_input":"2021-06-25T16:34:42.390373Z","iopub.status.idle":"2021-06-25T16:34:42.409187Z","shell.execute_reply.started":"2021-06-25T16:34:42.390325Z","shell.execute_reply":"2021-06-25T16:34:42.408017Z"},"trusted":true},"execution_count":10,"outputs":[{"name":"stdout","text":"TOXIC: I have a muslim friend\n","output_type":"stream"},{"execution_count":10,"output_type":"execute_result","data":{"text/plain":"         word     coeff\n21422  friend  0.056005\n24208    have -0.066235\n34763  muslim  1.759788","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>word</th>\n      <th>coeff</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>21422</th>\n      <td>friend</td>\n      <td>0.056005</td>\n    </tr>\n    <tr>\n      <th>24208</th>\n      <td>have</td>\n      <td>-0.066235</td>\n    </tr>\n    <tr>\n      <th>34763</th>\n      <td>muslim</td>\n      <td>1.759788</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"new_comment = \"My friend is black\"\n\n# Do not change the code below\nclassify_string(new_comment)","metadata":{"execution":{"iopub.status.busy":"2021-06-25T16:48:14.377038Z","iopub.execute_input":"2021-06-25T16:48:14.377368Z","iopub.status.idle":"2021-06-25T16:48:14.382287Z","shell.execute_reply.started":"2021-06-25T16:48:14.377341Z","shell.execute_reply":"2021-06-25T16:48:14.381506Z"},"trusted":true},"execution_count":21,"outputs":[{"name":"stdout","text":"TOXIC: My friend is black\n","output_type":"stream"}]},{"cell_type":"code","source":"new_comment = \"I'm gay\"\n\n# Do not change the code below\nclassify_string(new_comment)","metadata":{"execution":{"iopub.status.busy":"2021-06-25T16:50:51.041298Z","iopub.execute_input":"2021-06-25T16:50:51.041646Z","iopub.status.idle":"2021-06-25T16:50:51.046632Z","shell.execute_reply.started":"2021-06-25T16:50:51.041617Z","shell.execute_reply":"2021-06-25T16:50:51.045891Z"},"trusted":true},"execution_count":22,"outputs":[{"name":"stdout","text":"TOXIC: I'm gay\n","output_type":"stream"}]},{"cell_type":"markdown","source":"So, we can see how biased the model is.","metadata":{}},{"cell_type":"markdown","source":"So,Comments that refer to Islam are more likely to be classified as toxic, because of a flawed state of the online community where the data was collected. This can introduce historical bias.\n\nBeside, if we hypothesize that a model is being trained to classify online comments as toxic. So, any comments that are not in english,so trasnslated in English with a seperate tool. This can introduce since non-English comments will often not be translated perfectly.\n\nAdditionally,If the model is evaluated based on comments from users in the United Kingdom and deployed to users in Australia, this will lead to evaluation bias and deployment bias. The model will also have representation bias, because it was built to serve users in Australia, but was trained with data from users based in the United Kingdom.","metadata":{}}]}